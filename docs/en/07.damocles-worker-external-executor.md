# Configuration Example for damocles-worker External Executor

In other documents, we have introduced the basic concept of `processor` and analyzed the configuration fields related to `processor` in `damocles-worker`.   

Next, we will use an actual scenario as an example to see exactly how to orchestrate external processors.

## Case study

### Hardware

We are targeting a system with the following hardware:

- Dual `AMD EPYC 7642 48-Core Processor`
- 2T memory
- Two `GeForce RTX 3080` graphic cards
- Ample local NVME data drives

With this, the computation resources we can utilize are approximately:

- 96 CPU physical cores, or a total of 32 usable pc1 multicore groups at 3c/DIE
- About 1.96TiB usable physical memory
- 2 `GeForce RTX 3080` GPUs

### Allocation Scheme
With 32GiB CC sector sealing, there are two possible allocation schemes, elaborated below. 

**Note:** The following designs can be used as reference solutions but should not be directly applied in production environments. Users need to make adjustments based on actual conditions and output volumes during testing phases.

#### Scheme A: 28 pc1, 1 pc2, 1 c2, pc2 and c2 each group occupies a separate GPU

 The idea behind this scheme is to share one dedicated pc2 and c2 external executor each while satisfying consumption capabilities. 
 
 The `processors` section configuration is as follows:


```
[processors.limitation.concurrent]
pc1 = 28
pc2 = 1
c2 = 1

[[processors.pc1]]
numa_preferred = 0
cgroup.cpuset = "0-41"
concurrent = 14
envs = { FIL_PROOFS_MAXIMIZE_CACHING="1", FIL_PROOFS_USE_MULTICORE_SDR = "1", FIL_PROOFS_MULTICORE_SDR_PRODUCERS = "1" }

[[processors.pc1]]
numa_preferred = 1
cgroup.cpuset = "48-89"
concurrent = 14
envs = { FIL_PROOFS_MAXIMIZE_CACHING="1", FIL_PROOFS_USE_MULTICORE_SDR = "1", FIL_PROOFS_MULTICORE_SDR_PRODUCERS = "1" }

[[processors.pc2]]
cgroup.cpuset =  "2,5,8,11,50,53,56,59"
envs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }

[[processors.c2]]
cgroup.cpuset =  "14,17,20,23,26,29,32,35,42-47,62,65,68,71,74,77,80,83,90-95"
envs = { CUDA_VISIBLE_DEVICES = "1" }
```

Under this configuration, a total of 2 external `pc1` executors, 1 external `pc2` executor, and 1 external `c2` executor are started:

- 1 `pc1` external processor, specifies memory affinity to numa node 0, uses 1 main core + 1 Producer core configuration, allocates CPU cores `0-41` located in numa node 0;
- 1 `pc1` external processor, specifies memory affinity to numa node 1, uses 1 main core + 1 Producer core configuration, allocates CPU cores `48-89` located in numa node 1;
- 1 `pc2` external processor, uses CPU cores `2,5,8,11,50,53,56,59`, specifies GPU with visible index 0; this is feasible because in the current `pc1` configuration, each DIE will free up one core that can be used to perform lightweight computing tasks. With `pc2` using GPU, the CPU is basically only used for data transfer;
- 1 `c2` external processor using CPU cores `14,17,20,23,26,29,32,35,42-47,62,65,68,71,74,77,80,83,90-95`, specifies GPU with visible index 1;
- Remaining CPU cores `38,41,86,89` can be used for other lightweight tasks such as maintenance and management.

The advantage of this scheme is that `pc2` and `c2` each exclusively occupy resources, avoiding scheduling issues.

#### Scheme B: 28 pc1, 2 pc2, 2 c2, each pc2 paired with a c2 to form a group, occupying 1 GPU per group, with tasks in different stages within each group competing for resources.

The idea behind this scheme is to treat the dual-path hardware configuration as a combination of 2 sets of single-path hardware and apply the same configuration strategy to each single-path set. 

The configuration for the `processors` part is as follows:


```

[processors.limitation.concurrent]
pc1 = 28
pc2 = 2
c2 = 2

[processors.ext_locks]
gpu1 = 1
gpu2 = 1

[[processors.pc1]]
numa_preferred = 0
cgroup.cpuset = "0-41"
concurrent = 14
envs = { FIL_PROOFS_MAXIMIZE_CACHING="1", FIL_PROOFS_USE_MULTICORE_SDR = "1", FIL_PROOFS_MULTICORE_SDR_PRODUCERS = "1" }

[[processors.pc1]]
numa_preferred = 1
cgroup.cpuset = "48-89"
concurrent = 14
envs = { FIL_PROOFS_MAXIMIZE_CACHING="1", FIL_PROOFS_USE_MULTICORE_SDR = "1", FIL_PROOFS_MULTICORE_SDR_PRODUCERS = "1" }

[[processors.pc2]]
cgroup.cpuset =  "2,5,8,11,14,17,20,23"
locks = ["gpu1"]
envs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }

[[processors.pc2]]
cgroup.cpuset =  "50,53,56,59,62,65,68,71"
locks = ["gpu2"]
envs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "1" }

[[processors.c2]]
cgroup.cpuset =  "2,5,8,11,14,17,20,23,26,29,32,35,42-47"
locks = ["gpu1"]
envs = { CUDA_VISIBLE_DEVICES = "0" }

[[processors.c2]]
cgroup.cpuset =  "50,53,56,59,62,65,68,71,74,77,80,83,90-95"
locks = ["gpu2"]
envs = { CUDA_VISIBLE_DEVICES = "1" }
```

Under this configuration, a total of 2 external pc1 executors, 2 external pc2 executors, and 2 external c2 executors are started:

- 1 `pc1` external processor, specifies memory affinity to numa node 0, uses 1 main core + 1 Producer core configuration, allocates CPU cores `0-41` located in numa node 0;
- 1 `pc1` external processor, specifies memory affinity to numa node 1, uses 1 main core + 1 Producer core configuration, allocates CPU cores `48-89` located in numa node 1;
- 1 `pc2 `and 1 `c2` form a competing relationship around custom control lock `gpu1`, forming one group, where:
  - 1 `pc2`, using CPU cores `2,5,8,11,14,17,20,23`, specifying GPU with visible index 0;
  - 1 `c2`, using CPU cores `2,5,8,11,14,17,20,23,26,29,32,35,42-47`, specifying GPU with visible index 0; This is feasible because within this group, `pc2` and `c2` processors will not execute tasks simultaneously due to the custom control lock constraint, hence some CPU and GPU resources can be shared;

- 1 `pc2` and 1 `c2` form a competing relationship around custom control lock `gpu2`, forming one group, where:
  - 1 `pc2`, using CPU cores `50,53,56,59,62,65,68,71`, specifying GPU with visible index 1;
  - 1 `c2`, using CPU cores `50,53,56,59,62,65,68,71,74,77,80,83,90-95`, specifying GPU with visible index 1;

- Remaining CPU cores `38,41,86,89` can be used for other lightweight tasks like maintenance and management.

Compared to Scheme A, this scheme may result in an extreme situation where the control lock is continuously held by one type of task for a long time, preventing the other type of task from executing and leading to stalled sector consumption. 

In simple terms, GPU may be occupied by `pc2` execution for a long time and cannot be released to `c2`, resulting in backlogged sectors waiting for `c2` resources.

## Conclusion

What this document provides is *how to design an allocation scheme tailored to your own needs*, rather than *a one-size-fits-all ratio scheme*. 

We hope to provide more automatic configuration tools and calculators later on to simplify the scheme design process for users. However, we still recommend that users have a basic understanding of the key aspects of the scheme.
